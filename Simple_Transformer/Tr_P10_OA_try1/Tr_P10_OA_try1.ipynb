{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36039876-d3da-4e0b-8a22-87c58265c3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU 0 for the experiment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the CUDA_VISIBLE_DEVICES environment variable\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Replace with the GPU index you want to use\n",
    "\n",
    "# Confirm the selected GPU\n",
    "print(f\"Using GPU {os.environ['CUDA_VISIBLE_DEVICES']} for the experiment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59f489c7-68b3-46bd-8156-20d9ae289490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 21:23:41.227291: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "import math\n",
    "import json\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b27a8579-d50a-4e17-9fbf-bc013d2db8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras; print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "859b091e-a461-411b-898a-d877b4ea90ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46c3fefe-d02d-4022-9eb9-92b045e4cfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49730\n",
      "(49730, 50)\n",
      "12433\n",
      "(49730, 81)\n",
      "49730\n",
      "(12433, 50)\n",
      "12433\n",
      "(12433, 81)\n",
      "X_train_reshaped shape:  (49730, 50, 1)\n",
      "Y_train_resahped shape:  (49730, 81, 1)\n",
      "Input_shape:  (50, 1)\n",
      "X_test_reshaped (12433, 50, 1)\n",
      "Y_test_reshaped (12433, 81, 1)\n"
     ]
    }
   ],
   "source": [
    "#Prepare Data:\n",
    "path_to_X = \"../Normalised/X.npy\"\n",
    "path_to_Y = \"../Normalised/Y.npy\"\n",
    "X = np.load(path_to_X)\n",
    "Y = np.load(path_to_Y)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=42)\n",
    "print(len(X_train))\n",
    "print(X_train.shape)\n",
    "print(len(X_test))\n",
    "print(Y_train.shape)\n",
    "print(len(Y_train))\n",
    "print(X_test.shape)\n",
    "print(len(Y_test))\n",
    "print(Y_test.shape)\n",
    "\n",
    "# Reshape the input and output data for the encoder and decoder\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1) # (No of Data, 50) to (No of data, 50,1)\n",
    "print(\"X_train_reshaped shape: \", X_train_reshaped.shape)\n",
    "Y_train_reshaped = Y_train.reshape(Y_train.shape[0], Y_train.shape[1], 1)\n",
    "print(\"Y_train_resahped shape: \", Y_train_reshaped.shape)\n",
    "# Define the input shape for the encoder\n",
    "input_shape = X_train_reshaped.shape[1:]  # Shape: (sequence_length, 1) # 50,1\n",
    "print(\"Input_shape: \", input_shape)\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "Y_test_reshaped = Y_test.reshape(Y_test.shape[0], Y_test.shape[1], 1)\n",
    "print(\"X_test_reshaped\",X_test_reshaped.shape)\n",
    "print(\"Y_test_reshaped\",Y_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aae4ade-e7c9-4feb-bd8e-2a26f81542ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positional encoding is crucial for transformer models to understand the order of the elements in a sequence.\n",
    "#This helps the model understand the sequential order of the input data.\n",
    "def positional_encoding(seq_length, d_model):\n",
    "     #This creates a tensor of shape (seq_length, 1) representing the positions of tokens in the sequence:\n",
    "    pos = tf.range(seq_length, dtype=tf.float32)[:, tf.newaxis]\n",
    "    \n",
    "    #This creates an exponential term with different frequencies:\n",
    "    i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :]\n",
    "    angle_rates = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    angle_rads = pos * angle_rates\n",
    "\n",
    "    #The angles are split into sine and cosine components. This step is crucial for capturing different positional information.\n",
    "    # Apply sin to even indices in the array; 2i\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # Apply cos to odd indices in the array; 2i+1\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    #Sine and cosine values are concatenated along the last axis to create the final positional encoding for a single position.\n",
    "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "    #Finally, a batch dimension is added to the tensor to make it compatible with batched input sequences.\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "def transformer_block(units, heads, dropout, ff_dim, name):\n",
    "    #RM was: inputs = layers.Input(shape=(None, units)) #was this, may be incorrect\n",
    "    #change to below to define the input layer::\n",
    "    inputs = layers.Input(shape=(50, 1))  # Set the input shape explicitly\n",
    "\n",
    "    # Self-attention\n",
    "    #The transformer block uses a multi-head self-attention mechanism with the layers.MultiHeadAttention layer. \n",
    "    #RM was:attention = layers.MultiHeadAttention(key_dim=units // heads, num_heads=heads, dropout=dropout)(inputs, inputs)\n",
    "    #key_dim=1 The dimensionality of the key space. In this case, it's set to 1 since you're dealing with 1D sequences.\n",
    "    #num_heads=heads: The number of attention heads\n",
    "    #dropout=dropout: The dropout rate applied to attention weights during training to prevent overfitting.\n",
    "    #(inputs, inputs) In this case, the inputs are the same for both the query and key components of self-attention. \n",
    "    #This is common in self-attention mechanisms, where the input sequence itself is used to compute attention weights.\n",
    "    attention = layers.MultiHeadAttention(key_dim=1, num_heads=heads, dropout=dropout)(inputs, inputs)\n",
    "\n",
    "    #Dropout is applied after self-attention, and layer normalization is performed. \n",
    "    #These help with regularization and stabilizing training.\n",
    "    attention = layers.Dropout(rate=dropout)(attention)\n",
    "    attention = layers.LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    # Feed-forward\n",
    "    #Two 1D convolutional layers (Conv1D) are used for the feed-forward network.\n",
    "    # The first one reduces the dimensionality with a ReLU activation\n",
    "    #ReLU introduces non-linearity to the model, allowing it to learn more complex patterns in the data.\n",
    "    # Increasing ff_dim might enhance the model's capacity to capture intricate patterns,\n",
    "    # but it also comes with increased computational requirements.\n",
    "    ff = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(attention)\n",
    "    ff = layers.Dropout(rate=dropout)(ff)\n",
    "    #the second one restores the dimensionality. \n",
    "    ff = layers.Conv1D(filters=units, kernel_size=1)(ff)\n",
    "    # dropout and layer normalization are applied for regularization.\n",
    "    ff = layers.Dropout(rate=dropout)(ff)\n",
    "    ff = layers.LayerNormalization(epsilon=1e-6)(attention + ff)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=ff, name=name)\n",
    "    return model\n",
    "\n",
    "def build_model(units, heads, dropout, ff_dim, num_blocks, input_length=50, output_length=81):\n",
    "    # inputs = layers.Input(shape=(input_length, units))\n",
    "    inputs = layers.Input(shape=(input_length, 1))\n",
    "    x = inputs\n",
    "\n",
    "    pos_encoding = positional_encoding(input_length, units)\n",
    "    x = x + pos_encoding[:, :input_length, :]\n",
    "\n",
    "    #multiple transformer blocks are stacked\n",
    "    for i in range(num_blocks):\n",
    "        x = transformer_block(units=units, heads=heads, dropout=dropout, ff_dim=ff_dim, name=f\"transformer_block_{i}\")(x)\n",
    "\n",
    "    #After the transformer blocks, global average pooling is applied, \n",
    "    #followed by dropout and a dense layer with ReLU activation.\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(20, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    #Finally, a dense layer with a sigmoid activation function is used for the output layer,\n",
    "    #reason why we use sigmoid function is because it exists between (0 to 1)\n",
    "    outputs = layers.Dense(output_length, activation=\"sigmoid\")(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b8235bd-dad5-4111-9b32-a0ead6aabb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "units = 128\n",
    "heads = 4\n",
    "dropout = 0.2\n",
    "ff_dim = 128\n",
    "num_blocks = 3\n",
    "\n",
    "# # Create the model\n",
    "# model = build_model(units=units, heads=heads, dropout=dropout, ff_dim=ff_dim, num_blocks=num_blocks)\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error', metrics = ['mae'])\n",
    "# # Print model summary\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b301603-b8e3-4fd1-b6fc-e9b80baa0d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./Tr_P10_OA/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0be12bee-84a1-45d4-a084-ff91863573b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f449ca64-a1bc-426d-86a2-8cc8f9fb3716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 19:58:37.122566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78946 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:01:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50, 1)]           0         \n",
      "                                                                 \n",
      " tf.__operators__.add (TFOp  (None, 50, 128)           0         \n",
      " Lambda)                                                         \n",
      "                                                                 \n",
      " transformer_block_0 (Funct  (None, None, 128)         99584     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " transformer_block_1 (Funct  (None, None, 128)         99584     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " transformer_block_2 (Funct  (None, None, 128)         99584     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 128)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                2580      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 81)                1701      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 303033 (1.16 MB)\n",
      "Trainable params: 303033 (1.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 19:58:42.117605: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-11-15 19:58:42.504617: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-11-15 19:58:43.121925: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f1af0007310 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-15 19:58:43.122000: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0\n",
      "2023-11-15 19:58:43.131073: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-15 19:58:43.255192: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0734 - mae: 0.1453\n",
      "Epoch 1: val_loss improved from inf to 0.05665, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 28s 15ms/step - loss: 0.0734 - mae: 0.1453 - val_loss: 0.0566 - val_mae: 0.1017\n",
      "Epoch 2/200\n",
      "   4/1244 [..............................] - ETA: 22s - loss: 0.0578 - mae: 0.1135"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0549 - mae: 0.1083\n",
      "Epoch 2: val_loss improved from 0.05665 to 0.04961, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 21s 17ms/step - loss: 0.0549 - mae: 0.1083 - val_loss: 0.0496 - val_mae: 0.0908\n",
      "Epoch 3/200\n",
      "1241/1244 [============================>.] - ETA: 0s - loss: 0.0507 - mae: 0.1003\n",
      "Epoch 3: val_loss improved from 0.04961 to 0.04755, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0508 - mae: 0.1003 - val_loss: 0.0476 - val_mae: 0.0844\n",
      "Epoch 4/200\n",
      "1240/1244 [============================>.] - ETA: 0s - loss: 0.0488 - mae: 0.0966\n",
      "Epoch 4: val_loss improved from 0.04755 to 0.04590, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 17s 13ms/step - loss: 0.0488 - mae: 0.0966 - val_loss: 0.0459 - val_mae: 0.0838\n",
      "Epoch 5/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0478 - mae: 0.0943\n",
      "Epoch 5: val_loss improved from 0.04590 to 0.04455, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0478 - mae: 0.0943 - val_loss: 0.0446 - val_mae: 0.0807\n",
      "Epoch 6/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0467 - mae: 0.0921\n",
      "Epoch 6: val_loss improved from 0.04455 to 0.04240, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 18s 14ms/step - loss: 0.0466 - mae: 0.0921 - val_loss: 0.0424 - val_mae: 0.0760\n",
      "Epoch 7/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0459 - mae: 0.0904\n",
      "Epoch 7: val_loss did not improve from 0.04240\n",
      "1244/1244 [==============================] - 18s 15ms/step - loss: 0.0459 - mae: 0.0904 - val_loss: 0.0435 - val_mae: 0.0759\n",
      "Epoch 8/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0452 - mae: 0.0887\n",
      "Epoch 8: val_loss improved from 0.04240 to 0.04239, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0452 - mae: 0.0887 - val_loss: 0.0424 - val_mae: 0.0747\n",
      "Epoch 9/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0450 - mae: 0.0883\n",
      "Epoch 9: val_loss did not improve from 0.04239\n",
      "1244/1244 [==============================] - 18s 15ms/step - loss: 0.0450 - mae: 0.0883 - val_loss: 0.0434 - val_mae: 0.0747\n",
      "Epoch 10/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0445 - mae: 0.0870\n",
      "Epoch 10: val_loss improved from 0.04239 to 0.04187, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 16s 12ms/step - loss: 0.0445 - mae: 0.0871 - val_loss: 0.0419 - val_mae: 0.0734\n",
      "Epoch 11/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0441 - mae: 0.0861\n",
      "Epoch 11: val_loss improved from 0.04187 to 0.04071, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 16s 13ms/step - loss: 0.0441 - mae: 0.0861 - val_loss: 0.0407 - val_mae: 0.0712\n",
      "Epoch 12/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0434 - mae: 0.0848\n",
      "Epoch 12: val_loss did not improve from 0.04071\n",
      "1244/1244 [==============================] - 17s 14ms/step - loss: 0.0434 - mae: 0.0848 - val_loss: 0.0434 - val_mae: 0.0736\n",
      "Epoch 13/200\n",
      "1240/1244 [============================>.] - ETA: 0s - loss: 0.0435 - mae: 0.0849\n",
      "Epoch 13: val_loss did not improve from 0.04071\n",
      "1244/1244 [==============================] - 15s 12ms/step - loss: 0.0435 - mae: 0.0849 - val_loss: 0.0411 - val_mae: 0.0713\n",
      "Epoch 14/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0430 - mae: 0.0838\n",
      "Epoch 14: val_loss did not improve from 0.04071\n",
      "1244/1244 [==============================] - 15s 12ms/step - loss: 0.0430 - mae: 0.0838 - val_loss: 0.0413 - val_mae: 0.0722\n",
      "Epoch 15/200\n",
      "1241/1244 [============================>.] - ETA: 0s - loss: 0.0431 - mae: 0.0842\n",
      "Epoch 15: val_loss improved from 0.04071 to 0.03986, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 15s 12ms/step - loss: 0.0431 - mae: 0.0841 - val_loss: 0.0399 - val_mae: 0.0693\n",
      "Epoch 16/200\n",
      "1240/1244 [============================>.] - ETA: 0s - loss: 0.0429 - mae: 0.0836\n",
      "Epoch 16: val_loss did not improve from 0.03986\n",
      "1244/1244 [==============================] - 15s 12ms/step - loss: 0.0429 - mae: 0.0836 - val_loss: 0.0410 - val_mae: 0.0707\n",
      "Epoch 17/200\n",
      "1241/1244 [============================>.] - ETA: 0s - loss: 0.0429 - mae: 0.0835\n",
      "Epoch 17: val_loss did not improve from 0.03986\n",
      "1244/1244 [==============================] - 15s 12ms/step - loss: 0.0429 - mae: 0.0835 - val_loss: 0.0407 - val_mae: 0.0733\n",
      "Epoch 18/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0426 - mae: 0.0833\n",
      "Epoch 18: val_loss did not improve from 0.03986\n",
      "1244/1244 [==============================] - 16s 13ms/step - loss: 0.0426 - mae: 0.0833 - val_loss: 0.0411 - val_mae: 0.0711\n",
      "Epoch 19/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0426 - mae: 0.0831\n",
      "Epoch 19: val_loss improved from 0.03986 to 0.03957, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 15s 12ms/step - loss: 0.0426 - mae: 0.0831 - val_loss: 0.0396 - val_mae: 0.0694\n",
      "Epoch 20/200\n",
      "1241/1244 [============================>.] - ETA: 0s - loss: 0.0417 - mae: 0.0815\n",
      "Epoch 20: val_loss improved from 0.03957 to 0.03950, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 16s 13ms/step - loss: 0.0417 - mae: 0.0815 - val_loss: 0.0395 - val_mae: 0.0674\n",
      "Epoch 21/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0418 - mae: 0.0816\n",
      "Epoch 21: val_loss improved from 0.03950 to 0.03919, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 17s 14ms/step - loss: 0.0418 - mae: 0.0816 - val_loss: 0.0392 - val_mae: 0.0697\n",
      "Epoch 22/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0420 - mae: 0.0819\n",
      "Epoch 22: val_loss did not improve from 0.03919\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0420 - mae: 0.0819 - val_loss: 0.0396 - val_mae: 0.0709\n",
      "Epoch 23/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0417 - mae: 0.0811\n",
      "Epoch 23: val_loss did not improve from 0.03919\n",
      "1244/1244 [==============================] - 17s 14ms/step - loss: 0.0417 - mae: 0.0811 - val_loss: 0.0393 - val_mae: 0.0678\n",
      "Epoch 24/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0414 - mae: 0.0808\n",
      "Epoch 24: val_loss improved from 0.03919 to 0.03819, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 17s 14ms/step - loss: 0.0414 - mae: 0.0808 - val_loss: 0.0382 - val_mae: 0.0670\n",
      "Epoch 25/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0415 - mae: 0.0812\n",
      "Epoch 25: val_loss did not improve from 0.03819\n",
      "1244/1244 [==============================] - 18s 14ms/step - loss: 0.0415 - mae: 0.0812 - val_loss: 0.0392 - val_mae: 0.0694\n",
      "Epoch 26/200\n",
      "1241/1244 [============================>.] - ETA: 0s - loss: 0.0413 - mae: 0.0807\n",
      "Epoch 26: val_loss did not improve from 0.03819\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0413 - mae: 0.0807 - val_loss: 0.0393 - val_mae: 0.0685\n",
      "Epoch 27/200\n",
      "1241/1244 [============================>.] - ETA: 0s - loss: 0.0407 - mae: 0.0794\n",
      "Epoch 27: val_loss did not improve from 0.03819\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0407 - mae: 0.0794 - val_loss: 0.0384 - val_mae: 0.0661\n",
      "Epoch 28/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0411 - mae: 0.0799\n",
      "Epoch 28: val_loss did not improve from 0.03819\n",
      "1244/1244 [==============================] - 19s 15ms/step - loss: 0.0411 - mae: 0.0799 - val_loss: 0.0397 - val_mae: 0.0684\n",
      "Epoch 29/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0407 - mae: 0.0794\n",
      "Epoch 29: val_loss improved from 0.03819 to 0.03774, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 19s 15ms/step - loss: 0.0407 - mae: 0.0794 - val_loss: 0.0377 - val_mae: 0.0667\n",
      "Epoch 30/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0414 - mae: 0.0806\n",
      "Epoch 30: val_loss improved from 0.03774 to 0.03767, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 18s 15ms/step - loss: 0.0414 - mae: 0.0806 - val_loss: 0.0377 - val_mae: 0.0672\n",
      "Epoch 31/200\n",
      "1240/1244 [============================>.] - ETA: 0s - loss: 0.0404 - mae: 0.0790\n",
      "Epoch 31: val_loss did not improve from 0.03767\n",
      "1244/1244 [==============================] - 16s 13ms/step - loss: 0.0404 - mae: 0.0790 - val_loss: 0.0383 - val_mae: 0.0680\n",
      "Epoch 32/200\n",
      "1241/1244 [============================>.] - ETA: 0s - loss: 0.0405 - mae: 0.0790\n",
      "Epoch 32: val_loss did not improve from 0.03767\n",
      "1244/1244 [==============================] - 17s 13ms/step - loss: 0.0405 - mae: 0.0790 - val_loss: 0.0381 - val_mae: 0.0664\n",
      "Epoch 33/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0417 - mae: 0.0813\n",
      "Epoch 33: val_loss did not improve from 0.03767\n",
      "1244/1244 [==============================] - 19s 15ms/step - loss: 0.0417 - mae: 0.0813 - val_loss: 0.0412 - val_mae: 0.0692\n",
      "Epoch 34/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0424 - mae: 0.0824\n",
      "Epoch 34: val_loss did not improve from 0.03767\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0424 - mae: 0.0824 - val_loss: 0.0380 - val_mae: 0.0675\n",
      "Epoch 35/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0401 - mae: 0.0783\n",
      "Epoch 35: val_loss improved from 0.03767 to 0.03695, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0401 - mae: 0.0783 - val_loss: 0.0370 - val_mae: 0.0663\n",
      "Epoch 36/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0400 - mae: 0.0780\n",
      "Epoch 36: val_loss did not improve from 0.03695\n",
      "1244/1244 [==============================] - 21s 17ms/step - loss: 0.0400 - mae: 0.0780 - val_loss: 0.0381 - val_mae: 0.0675\n",
      "Epoch 37/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0396 - mae: 0.0773\n",
      "Epoch 37: val_loss did not improve from 0.03695\n",
      "1244/1244 [==============================] - 19s 15ms/step - loss: 0.0396 - mae: 0.0773 - val_loss: 0.0375 - val_mae: 0.0659\n",
      "Epoch 38/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0398 - mae: 0.0776\n",
      "Epoch 38: val_loss improved from 0.03695 to 0.03662, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 21s 17ms/step - loss: 0.0398 - mae: 0.0776 - val_loss: 0.0366 - val_mae: 0.0658\n",
      "Epoch 39/200\n",
      "1240/1244 [============================>.] - ETA: 0s - loss: 0.0392 - mae: 0.0765\n",
      "Epoch 39: val_loss did not improve from 0.03662\n",
      "1244/1244 [==============================] - 18s 15ms/step - loss: 0.0392 - mae: 0.0765 - val_loss: 0.0381 - val_mae: 0.0669\n",
      "Epoch 40/200\n",
      "1240/1244 [============================>.] - ETA: 0s - loss: 0.0395 - mae: 0.0771\n",
      "Epoch 40: val_loss did not improve from 0.03662\n",
      "1244/1244 [==============================] - 16s 13ms/step - loss: 0.0395 - mae: 0.0771 - val_loss: 0.0375 - val_mae: 0.0669\n",
      "Epoch 41/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0389 - mae: 0.0758\n",
      "Epoch 41: val_loss did not improve from 0.03662\n",
      "1244/1244 [==============================] - 17s 14ms/step - loss: 0.0389 - mae: 0.0758 - val_loss: 0.0382 - val_mae: 0.0681\n",
      "Epoch 42/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0389 - mae: 0.0757\n",
      "Epoch 42: val_loss did not improve from 0.03662\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0389 - mae: 0.0757 - val_loss: 0.0382 - val_mae: 0.0680\n",
      "Epoch 43/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0397 - mae: 0.0771\n",
      "Epoch 43: val_loss improved from 0.03662 to 0.03607, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0397 - mae: 0.0771 - val_loss: 0.0361 - val_mae: 0.0645\n",
      "Epoch 44/200\n",
      "1240/1244 [============================>.] - ETA: 0s - loss: 0.0393 - mae: 0.0765\n",
      "Epoch 44: val_loss did not improve from 0.03607\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0393 - mae: 0.0765 - val_loss: 0.0378 - val_mae: 0.0655\n",
      "Epoch 45/200\n",
      "1241/1244 [============================>.] - ETA: 0s - loss: 0.0398 - mae: 0.0772\n",
      "Epoch 45: val_loss did not improve from 0.03607\n",
      "1244/1244 [==============================] - 19s 16ms/step - loss: 0.0398 - mae: 0.0772 - val_loss: 0.0390 - val_mae: 0.0685\n",
      "Epoch 46/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0386 - mae: 0.0751\n",
      "Epoch 46: val_loss improved from 0.03607 to 0.03597, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 19s 16ms/step - loss: 0.0386 - mae: 0.0751 - val_loss: 0.0360 - val_mae: 0.0643\n",
      "Epoch 47/200\n",
      "1240/1244 [============================>.] - ETA: 0s - loss: 0.0380 - mae: 0.0740\n",
      "Epoch 47: val_loss improved from 0.03597 to 0.03557, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 19s 15ms/step - loss: 0.0380 - mae: 0.0740 - val_loss: 0.0356 - val_mae: 0.0636\n",
      "Epoch 48/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0389 - mae: 0.0755\n",
      "Epoch 48: val_loss did not improve from 0.03557\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0389 - mae: 0.0755 - val_loss: 0.0362 - val_mae: 0.0649\n",
      "Epoch 49/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0382 - mae: 0.0744\n",
      "Epoch 49: val_loss did not improve from 0.03557\n",
      "1244/1244 [==============================] - 22s 17ms/step - loss: 0.0382 - mae: 0.0744 - val_loss: 0.0371 - val_mae: 0.0651\n",
      "Epoch 50/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0379 - mae: 0.0738\n",
      "Epoch 50: val_loss improved from 0.03557 to 0.03470, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 21s 17ms/step - loss: 0.0379 - mae: 0.0738 - val_loss: 0.0347 - val_mae: 0.0624\n",
      "Epoch 51/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0385 - mae: 0.0747\n",
      "Epoch 51: val_loss did not improve from 0.03470\n",
      "1244/1244 [==============================] - 21s 17ms/step - loss: 0.0385 - mae: 0.0747 - val_loss: 0.0362 - val_mae: 0.0645\n",
      "Epoch 52/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0376 - mae: 0.0733\n",
      "Epoch 52: val_loss did not improve from 0.03470\n",
      "1244/1244 [==============================] - 21s 17ms/step - loss: 0.0376 - mae: 0.0733 - val_loss: 0.0370 - val_mae: 0.0651\n",
      "Epoch 53/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0381 - mae: 0.0739\n",
      "Epoch 53: val_loss did not improve from 0.03470\n",
      "1244/1244 [==============================] - 21s 17ms/step - loss: 0.0381 - mae: 0.0739 - val_loss: 0.0364 - val_mae: 0.0652\n",
      "Epoch 54/200\n",
      "1241/1244 [============================>.] - ETA: 0s - loss: 0.0372 - mae: 0.0724\n",
      "Epoch 54: val_loss improved from 0.03470 to 0.03453, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0372 - mae: 0.0724 - val_loss: 0.0345 - val_mae: 0.0612\n",
      "Epoch 55/200\n",
      "1241/1244 [============================>.] - ETA: 0s - loss: 0.0376 - mae: 0.0731\n",
      "Epoch 55: val_loss did not improve from 0.03453\n",
      "1244/1244 [==============================] - 21s 16ms/step - loss: 0.0376 - mae: 0.0731 - val_loss: 0.0357 - val_mae: 0.0632\n",
      "Epoch 56/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0373 - mae: 0.0724\n",
      "Epoch 56: val_loss did not improve from 0.03453\n",
      "1244/1244 [==============================] - 19s 16ms/step - loss: 0.0373 - mae: 0.0724 - val_loss: 0.0350 - val_mae: 0.0632\n",
      "Epoch 57/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0369 - mae: 0.0715\n",
      "Epoch 57: val_loss improved from 0.03453 to 0.03373, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0368 - mae: 0.0715 - val_loss: 0.0337 - val_mae: 0.0614\n",
      "Epoch 58/200\n",
      "1241/1244 [============================>.] - ETA: 0s - loss: 0.0369 - mae: 0.0717\n",
      "Epoch 58: val_loss did not improve from 0.03373\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0369 - mae: 0.0717 - val_loss: 0.0337 - val_mae: 0.0612\n",
      "Epoch 59/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0372 - mae: 0.0721\n",
      "Epoch 59: val_loss did not improve from 0.03373\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0372 - mae: 0.0721 - val_loss: 0.0354 - val_mae: 0.0622\n",
      "Epoch 60/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0370 - mae: 0.0716\n",
      "Epoch 60: val_loss did not improve from 0.03373\n",
      "1244/1244 [==============================] - 19s 15ms/step - loss: 0.0370 - mae: 0.0716 - val_loss: 0.0342 - val_mae: 0.0626\n",
      "Epoch 61/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0373 - mae: 0.0719\n",
      "Epoch 61: val_loss did not improve from 0.03373\n",
      "1244/1244 [==============================] - 21s 17ms/step - loss: 0.0373 - mae: 0.0719 - val_loss: 0.0350 - val_mae: 0.0629\n",
      "Epoch 62/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0364 - mae: 0.0707\n",
      "Epoch 62: val_loss did not improve from 0.03373\n",
      "1244/1244 [==============================] - 18s 15ms/step - loss: 0.0364 - mae: 0.0707 - val_loss: 0.0340 - val_mae: 0.0617\n",
      "Epoch 63/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0368 - mae: 0.0712\n",
      "Epoch 63: val_loss improved from 0.03373 to 0.03355, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 21s 17ms/step - loss: 0.0367 - mae: 0.0712 - val_loss: 0.0335 - val_mae: 0.0603\n",
      "Epoch 64/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0372 - mae: 0.0718\n",
      "Epoch 64: val_loss did not improve from 0.03355\n",
      "1244/1244 [==============================] - 21s 17ms/step - loss: 0.0372 - mae: 0.0718 - val_loss: 0.0382 - val_mae: 0.0662\n",
      "Epoch 65/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0372 - mae: 0.0720\n",
      "Epoch 65: val_loss did not improve from 0.03355\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0372 - mae: 0.0720 - val_loss: 0.0341 - val_mae: 0.0624\n",
      "Epoch 66/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0366 - mae: 0.0710\n",
      "Epoch 66: val_loss did not improve from 0.03355\n",
      "1244/1244 [==============================] - 17s 14ms/step - loss: 0.0366 - mae: 0.0710 - val_loss: 0.0342 - val_mae: 0.0623\n",
      "Epoch 67/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0371 - mae: 0.0718\n",
      "Epoch 67: val_loss did not improve from 0.03355\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0371 - mae: 0.0718 - val_loss: 0.0348 - val_mae: 0.0621\n",
      "Epoch 68/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0368 - mae: 0.0713\n",
      "Epoch 68: val_loss did not improve from 0.03355\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0368 - mae: 0.0713 - val_loss: 0.0406 - val_mae: 0.0717\n",
      "Epoch 69/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0382 - mae: 0.0739\n",
      "Epoch 69: val_loss improved from 0.03355 to 0.03301, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 21s 17ms/step - loss: 0.0382 - mae: 0.0739 - val_loss: 0.0330 - val_mae: 0.0613\n",
      "Epoch 70/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0360 - mae: 0.0701\n",
      "Epoch 70: val_loss did not improve from 0.03301\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0360 - mae: 0.0701 - val_loss: 0.0333 - val_mae: 0.0600\n",
      "Epoch 71/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0695\n",
      "Epoch 71: val_loss did not improve from 0.03301\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0359 - mae: 0.0695 - val_loss: 0.0349 - val_mae: 0.0620\n",
      "Epoch 72/200\n",
      "1241/1244 [============================>.] - ETA: 0s - loss: 0.0364 - mae: 0.0701\n",
      "Epoch 72: val_loss did not improve from 0.03301\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0365 - mae: 0.0701 - val_loss: 0.0346 - val_mae: 0.0633\n",
      "Epoch 73/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0358 - mae: 0.0694\n",
      "Epoch 73: val_loss did not improve from 0.03301\n",
      "1244/1244 [==============================] - 22s 17ms/step - loss: 0.0358 - mae: 0.0694 - val_loss: 0.0333 - val_mae: 0.0599\n",
      "Epoch 74/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0354 - mae: 0.0688\n",
      "Epoch 74: val_loss improved from 0.03301 to 0.03272, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 22s 17ms/step - loss: 0.0354 - mae: 0.0688 - val_loss: 0.0327 - val_mae: 0.0601\n",
      "Epoch 75/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0353 - mae: 0.0685\n",
      "Epoch 75: val_loss did not improve from 0.03272\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0353 - mae: 0.0685 - val_loss: 0.0333 - val_mae: 0.0596\n",
      "Epoch 76/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0364 - mae: 0.0704\n",
      "Epoch 76: val_loss did not improve from 0.03272\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0364 - mae: 0.0704 - val_loss: 0.0328 - val_mae: 0.0600\n",
      "Epoch 77/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0352 - mae: 0.0681\n",
      "Epoch 77: val_loss did not improve from 0.03272\n",
      "1244/1244 [==============================] - 19s 16ms/step - loss: 0.0352 - mae: 0.0681 - val_loss: 0.0332 - val_mae: 0.0602\n",
      "Epoch 78/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0363 - mae: 0.0699\n",
      "Epoch 78: val_loss did not improve from 0.03272\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0363 - mae: 0.0699 - val_loss: 0.0337 - val_mae: 0.0615\n",
      "Epoch 79/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0378 - mae: 0.0726\n",
      "Epoch 79: val_loss did not improve from 0.03272\n",
      "1244/1244 [==============================] - 21s 17ms/step - loss: 0.0378 - mae: 0.0726 - val_loss: 0.0351 - val_mae: 0.0636\n",
      "Epoch 80/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0371 - mae: 0.0718\n",
      "Epoch 80: val_loss did not improve from 0.03272\n",
      "1244/1244 [==============================] - 19s 15ms/step - loss: 0.0371 - mae: 0.0718 - val_loss: 0.0334 - val_mae: 0.0605\n",
      "Epoch 81/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0354 - mae: 0.0689\n",
      "Epoch 81: val_loss improved from 0.03272 to 0.03259, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 18s 15ms/step - loss: 0.0354 - mae: 0.0689 - val_loss: 0.0326 - val_mae: 0.0595\n",
      "Epoch 82/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0352 - mae: 0.0681\n",
      "Epoch 82: val_loss improved from 0.03259 to 0.03242, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0352 - mae: 0.0681 - val_loss: 0.0324 - val_mae: 0.0592\n",
      "Epoch 83/200\n",
      "1241/1244 [============================>.] - ETA: 0s - loss: 0.0352 - mae: 0.0681\n",
      "Epoch 83: val_loss did not improve from 0.03242\n",
      "1244/1244 [==============================] - 19s 15ms/step - loss: 0.0352 - mae: 0.0681 - val_loss: 0.0329 - val_mae: 0.0597\n",
      "Epoch 84/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0351 - mae: 0.0677\n",
      "Epoch 84: val_loss did not improve from 0.03242\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0351 - mae: 0.0677 - val_loss: 0.0328 - val_mae: 0.0608\n",
      "Epoch 85/200\n",
      "1240/1244 [============================>.] - ETA: 0s - loss: 0.0364 - mae: 0.0699\n",
      "Epoch 85: val_loss did not improve from 0.03242\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0364 - mae: 0.0699 - val_loss: 0.0448 - val_mae: 0.0770\n",
      "Epoch 86/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0386 - mae: 0.0741\n",
      "Epoch 86: val_loss did not improve from 0.03242\n",
      "1244/1244 [==============================] - 17s 14ms/step - loss: 0.0386 - mae: 0.0741 - val_loss: 0.0399 - val_mae: 0.0699\n",
      "Epoch 87/200\n",
      "1241/1244 [============================>.] - ETA: 0s - loss: 0.0356 - mae: 0.0691\n",
      "Epoch 87: val_loss did not improve from 0.03242\n",
      "1244/1244 [==============================] - 18s 14ms/step - loss: 0.0356 - mae: 0.0691 - val_loss: 0.0329 - val_mae: 0.0597\n",
      "Epoch 88/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0348 - mae: 0.0675\n",
      "Epoch 88: val_loss did not improve from 0.03242\n",
      "1244/1244 [==============================] - 19s 15ms/step - loss: 0.0348 - mae: 0.0675 - val_loss: 0.0334 - val_mae: 0.0598\n",
      "Epoch 89/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0347 - mae: 0.0670\n",
      "Epoch 89: val_loss did not improve from 0.03242\n",
      "1244/1244 [==============================] - 19s 15ms/step - loss: 0.0347 - mae: 0.0670 - val_loss: 0.0344 - val_mae: 0.0634\n",
      "Epoch 90/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0351 - mae: 0.0677\n",
      "Epoch 90: val_loss improved from 0.03242 to 0.03184, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 19s 15ms/step - loss: 0.0351 - mae: 0.0677 - val_loss: 0.0318 - val_mae: 0.0583\n",
      "Epoch 91/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0345 - mae: 0.0667\n",
      "Epoch 91: val_loss did not improve from 0.03184\n",
      "1244/1244 [==============================] - 21s 17ms/step - loss: 0.0345 - mae: 0.0667 - val_loss: 0.0325 - val_mae: 0.0594\n",
      "Epoch 92/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0356 - mae: 0.0685\n",
      "Epoch 92: val_loss did not improve from 0.03184\n",
      "1244/1244 [==============================] - 21s 17ms/step - loss: 0.0356 - mae: 0.0685 - val_loss: 0.0360 - val_mae: 0.0657\n",
      "Epoch 93/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0352 - mae: 0.0680\n",
      "Epoch 93: val_loss did not improve from 0.03184\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0352 - mae: 0.0680 - val_loss: 0.0327 - val_mae: 0.0597\n",
      "Epoch 94/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0354 - mae: 0.0682\n",
      "Epoch 94: val_loss did not improve from 0.03184\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0353 - mae: 0.0682 - val_loss: 0.0327 - val_mae: 0.0594\n",
      "Epoch 95/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0356 - mae: 0.0684\n",
      "Epoch 95: val_loss did not improve from 0.03184\n",
      "1244/1244 [==============================] - 21s 17ms/step - loss: 0.0356 - mae: 0.0684 - val_loss: 0.0332 - val_mae: 0.0601\n",
      "Epoch 96/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0356 - mae: 0.0685\n",
      "Epoch 96: val_loss did not improve from 0.03184\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0356 - mae: 0.0685 - val_loss: 0.0328 - val_mae: 0.0619\n",
      "Epoch 97/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0354 - mae: 0.0680\n",
      "Epoch 97: val_loss did not improve from 0.03184\n",
      "1244/1244 [==============================] - 21s 17ms/step - loss: 0.0354 - mae: 0.0680 - val_loss: 0.0338 - val_mae: 0.0613\n",
      "Epoch 98/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0345 - mae: 0.0667\n",
      "Epoch 98: val_loss did not improve from 0.03184\n",
      "1244/1244 [==============================] - 21s 17ms/step - loss: 0.0345 - mae: 0.0667 - val_loss: 0.0322 - val_mae: 0.0586\n",
      "Epoch 99/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0346 - mae: 0.0667\n",
      "Epoch 99: val_loss did not improve from 0.03184\n",
      "1244/1244 [==============================] - 22s 18ms/step - loss: 0.0346 - mae: 0.0667 - val_loss: 0.0322 - val_mae: 0.0586\n",
      "Epoch 100/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0340 - mae: 0.0657\n",
      "Epoch 100: val_loss improved from 0.03184 to 0.03163, saving model to ./Tr_P10_OA/best_tr_model.h5\n",
      "1244/1244 [==============================] - 21s 17ms/step - loss: 0.0340 - mae: 0.0657 - val_loss: 0.0316 - val_mae: 0.0574\n",
      "Epoch 101/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0349 - mae: 0.0669\n",
      "Epoch 101: val_loss did not improve from 0.03163\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0349 - mae: 0.0669 - val_loss: 0.0448 - val_mae: 0.0772\n",
      "Epoch 102/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0417 - mae: 0.0789\n",
      "Epoch 102: val_loss did not improve from 0.03163\n",
      "1244/1244 [==============================] - 19s 15ms/step - loss: 0.0417 - mae: 0.0789 - val_loss: 0.0480 - val_mae: 0.0824\n",
      "Epoch 103/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0402 - mae: 0.0772\n",
      "Epoch 103: val_loss did not improve from 0.03163\n",
      "1244/1244 [==============================] - 19s 15ms/step - loss: 0.0402 - mae: 0.0772 - val_loss: 0.0470 - val_mae: 0.0846\n",
      "Epoch 104/200\n",
      "1241/1244 [============================>.] - ETA: 0s - loss: 0.0406 - mae: 0.0779\n",
      "Epoch 104: val_loss did not improve from 0.03163\n",
      "1244/1244 [==============================] - 21s 17ms/step - loss: 0.0405 - mae: 0.0779 - val_loss: 0.0450 - val_mae: 0.0773\n",
      "Epoch 105/200\n",
      "1242/1244 [============================>.] - ETA: 0s - loss: 0.0409 - mae: 0.0785\n",
      "Epoch 105: val_loss did not improve from 0.03163\n",
      "1244/1244 [==============================] - 21s 17ms/step - loss: 0.0409 - mae: 0.0785 - val_loss: 0.0414 - val_mae: 0.0743\n",
      "Epoch 106/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0392 - mae: 0.0758\n",
      "Epoch 106: val_loss did not improve from 0.03163\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0392 - mae: 0.0758 - val_loss: 0.0384 - val_mae: 0.0694\n",
      "Epoch 107/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0364 - mae: 0.0708\n",
      "Epoch 107: val_loss did not improve from 0.03163\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0364 - mae: 0.0708 - val_loss: 0.0332 - val_mae: 0.0595\n",
      "Epoch 108/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0345 - mae: 0.0671\n",
      "Epoch 108: val_loss did not improve from 0.03163\n",
      "1244/1244 [==============================] - 20s 16ms/step - loss: 0.0345 - mae: 0.0671 - val_loss: 0.0330 - val_mae: 0.0585\n",
      "Epoch 109/200\n",
      "1243/1244 [============================>.] - ETA: 0s - loss: 0.0342 - mae: 0.0662\n",
      "Epoch 109: val_loss did not improve from 0.03163\n",
      "1244/1244 [==============================] - 21s 17ms/step - loss: 0.0342 - mae: 0.0662 - val_loss: 0.0324 - val_mae: 0.0580\n",
      "Epoch 110/200\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.0343 - mae: 0.0663Restoring model weights from the end of the best epoch: 100.\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.03163\n",
      "1244/1244 [==============================] - 19s 15ms/step - loss: 0.0343 - mae: 0.0663 - val_loss: 0.0322 - val_mae: 0.0580\n",
      "Epoch 110: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Define EarlyStopping and ModelCheckpoint callbacks\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "# model_checkpoint = tf.keras.callbacks.ModelCheckpoint(save_path+'best_tr_model.h5', save_best_only=True, verbose=1)\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 200\n",
    "\n",
    "# X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "# Y_train_tf = tf.convert_to_tensor(Y_train, dtype=tf.float32)\n",
    "\n",
    "# history = model.fit(\n",
    "#     X_train_tf, Y_train_tf,\n",
    "#     epochs=num_epochs,\n",
    "#     batch_size = 32,\n",
    "#     validation_split=0.2,  # 20% of the data will be used for validation\n",
    "#     callbacks=[early_stopping, model_checkpoint]\n",
    "# )\n",
    "\n",
    "#Train the model\n",
    "# Enable logging of device placement\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "\n",
    "# Verify the available GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"GPUs:\", gpus)\n",
    "if gpus:\n",
    "    # Use the first GPU for training\n",
    "    with tf.device('/GPU:0'):\n",
    "        # Create the model\n",
    "        model = build_model(units=units, heads=heads, dropout=dropout, ff_dim=ff_dim, num_blocks=num_blocks)\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error', metrics = ['mae'])\n",
    "        # Print model summary\n",
    "        model.summary()\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "        model_checkpoint = tf.keras.callbacks.ModelCheckpoint(save_path+'best_tr_model.h5', save_best_only=True, verbose=1)\n",
    "        \n",
    "        # Training loop\n",
    "        num_epochs = 200\n",
    "        \n",
    "        X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "        Y_train_tf = tf.convert_to_tensor(Y_train, dtype=tf.float32)\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train_reshaped, Y_train_reshaped,\n",
    "            epochs=num_epochs,\n",
    "            batch_size = 32,\n",
    "            validation_split=0.2,  # 20% of the data will be used for validation\n",
    "            callbacks=[early_stopping, model_checkpoint]\n",
    "        )\n",
    "        \n",
    "else:\n",
    "    print(\"No GPU available.\")\n",
    "\n",
    "# Save the history as a JSON file\n",
    "with open(save_path+'history.json','w') as file:\n",
    "            json.dump(history.history, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c68fbb-d89a-4eb3-8bfd-e659011bf9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(save_path+\"history.json\")\n",
    "history = json.load(f)\n",
    "# summarize history for loss\n",
    "mod_name = \"Tr_P10_OA\"\n",
    "plt.plot(history['loss'], label='Training Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Transformer '+mod_name+' training and validation loss over time')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "#plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32fd10e-bfd3-4e90-b448-40446a16662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model = tf.keras.models.load_model(save_path+'best_tr_model.h5')\n",
    "\n",
    "# Evaluation on the test set\n",
    "best_model.evaluate(test_dataset)\n",
    "# Generate predictions on the test data\n",
    "predictions = model.predict(X_test_tf)\n",
    "\n",
    "# Save the predictions and ground_truth as a NumPy array\n",
    "np.save(save_path+'predictions.npy', predictions)\n",
    "#Save the ground truth:\n",
    "np.save(save_path+\"ground-truth.npy\", Y_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(Y_test, predictions)\n",
    "mae = mean_absolute_error(Y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f590de42-2625-4b16-b2c5-57b3edc5b8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print some samples\n",
    "y_pred = np.load(save_path+\"predictions.npy\")\n",
    "y_true = np.load(save_path +\"ground-truth.npy\")\n",
    "\n",
    "print(y_pred.shape)\n",
    "print(y_true.shape)\n",
    "#Print some samples:\n",
    "for sample in range(5):\n",
    "    print(f\"SAMPLE: {sample}\")\n",
    "    for i in range (81):\n",
    "        print(f\"Index {i} Ground truth: {y_true[sample][i]} Predicted: {y_pred[sample][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b841a8-03c0-4acd-b2ac-7c9ee00e9b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Benesh convert\n",
    "#Error Array\n",
    "#Box Plot\n",
    "#MAPE\n",
    "#R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf31fc29-5a92-4e15-a227-50d51684a8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
